# # Install required libraries
# !pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git
# !pip install -q datasets bitsandbytes einops
# !pip install -q wandb

# Import required libraries
from datasets import load_dataset, Dataset
from random import randrange
import torch
from transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM
from trl import SFTTrainer
from huggingface_hub import login
import wandb
from huggingface_hub import notebook_login


import torch
from transformers import LlamaTokenizer, LlamaForCausalLM

tokenizer = LlamaTokenizer.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base')
model = LlamaForCausalLM.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base', torch_dtype=torch.bfloat16)


# Define a function to extract and tokenize the input text and response
def extract_and_tokenize(row):
    input_text = row['text'].split('\\n')[0]
    response = row['text'].split('\\n')[1]
    
    input_ids = tokenizer.encode_plus(
        input_text,
        max_length=169,  # adjust the max length as needed
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )['input_ids'].flatten()
    
    response_ids = tokenizer.encode_plus(
        response,
        max_length=158,  # adjust the max length as needed
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )['input_ids'].flatten()
    
    return input_ids, response_ids

import pandas as pd
df=pd.read_csv("test_sft.csv")
# Apply the function to each row in the dataframe
df['input_ids'], df['response_ids'] = zip(*df.apply(extract_and_tokenize, axis=1))

df['input_ids'] = df['input_ids'].apply(lambda x: x.tolist())
df['response_ids'] = df['response_ids'].apply(lambda x: x.tolist())



# Load dataset

dataset = Dataset.from_pandas(df)  # assuming your dataset is stored in a Pandas dataframe df

# # Apply the function to each row in the dataset
# dataset = dataset.map(extract_and_tokenize)

# Set model and tokenizer
# model_name = "sarvamai/OpenHathi-7B-Hi-v0.1-Base"
# tokenizer = LlamaTokenizer.from_pretrained(model_name)
# model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)

# Set LoRA and BitsAndBytes configurations
peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16",
)

# Log in to HF Hub
notebook_login()

# Log in to W&B
wandb.login()

# Set environment variable for W&B project
wandb.init(
    # set the wandb project where this run will be logged
    project="my-awesome-project",

    # track hyperparameters and run metadata
    config={
    "learning_rate": 0.02,
    "architecture": "CNN",
    "dataset": "CIFAR-100",
    "epochs": 10,
    }
)

%env WANDB_PROJECT=your_wandb_project_name  # replace with your W&B project name

# Set training arguments
trainingArgs = TrainingArguments(
    output_dir="finetuned_model",  # replace with your desired output dir
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    gradient_checkpointing=True,
    optim="paged_adamw_32bit",
    logging_steps=5,
    save_strategy="epoch",
    learning_rate=2e-4,
    weight_decay=0.001,
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    group_by_length=False,
    lr_scheduler_type="cosine",
    disable_tqdm=True,
    #report_to="wandb",
    seed=42,
)

# Create the trainer
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    max_seq_length=2048,
    tokenizer=tokenizer,
    packing=True,
    formatting_func=lambda x: x,  # no formatting function needed in this case
    args=trainingArgs,
)

# Train the model
trainer.train()
