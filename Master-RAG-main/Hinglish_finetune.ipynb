{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'advanced-rag'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Mileage bataani thi\" (Hindi)']\n",
      "['\"Purana hi accha nigga\" (Hindi and English)', '', \"Note: I've included the profanity/disrespectful word in the translation, as per your instructions. However, I strongly advise against using such language, as it can be offensive and disrespectful to others.\"]\n",
      "['\"Daddy\" ‡§ï‡§æ ‡§®‡§æ‡§Æ \"Pulsar 220\" üòÇüòÇüòÇ', '', 'In this case, \"Daddy\" is being used as a term of endearment or admiration towards the Pulsar 220 motorcycle, so I have translated the rest of the sentence accordingly.']\n",
      "['For the given comment \"New one looks like ninja 400\", I will leave the English words as they are and provide the rest in Hindi:', '', '\"Naya ek jaisa hai ninja 400\"']\n",
      "[\"Sure, I'll do my best to translate the comments from a mix of English and Indian languages to their original language script. Here are some examples:\", '', 'Comment: \"mere naam hai Roshan\"', 'Output: \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à ‡§∞‡•ã‡§∂‡§®\"', '', 'Comment: \"nenu UST lo Panichestunna\"', 'Output: \"‡∞®‡±á‡∞®‡±Å UST ‡∞≤‡±ã ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ\"', '', 'Comment: \"my age is 25 years\"', 'Output: \"my age is 25 years\"', '', 'Comment: \"aapka naam kya hai?\"', 'Output: \"‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?\"', '', 'Comment: \"sundar kaandam\"', 'Output: \"‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§ï‡§æ‡§Ç‡§°‡§Æ\"', '', 'Comment: \"Your comment is very bad\"', 'Output: \"Your comment is very bad\"', '', 'Comment: \"tum log aise comments mat do\"', 'Output: \"‡§§‡•Å‡§Æ ‡§≤‡•ã‡§ó ‡§ê‡§∏‡•á comments ‡§Æ‡§§ ‡§¶‡•ã\"', '', 'Comment: \"sukaam hai aapki madam\"', 'Output: \"‡§∏‡•Å‡§ï‡§æ‡§Æ ‡§π‡•à ‡§Ü‡§™‡§ï‡•Ä madam\"', '', 'I hope this helps! Let me know if you have any other questions.']\n",
      "['\"purana jaisa safeda hai lekin woh gold jaisa maanegaa\"', '', '(Note: This is a translation of the English phrase \"old is gold\" in the comment. The rest of the sentence is already in English.)']\n",
      "['\"Old ‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä ‡§®‡§Ø‡§æ ‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä\" (Hindi script)', '', 'Note: The phrase \"I dope\" was translated to \"‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä\" which means \"I am addicted\" or \"I am fond of\" in Hindi. The rest of the phrase was translated as is, since it is in English.']\n",
      "['The old Karizma has something the new one doesn\\'t, a soul : \"‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§ï‡§æ‡§∞‡§ø‡§ú‡§Æ‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§π‡•à ‡§ú‡•ã ‡§®‡§è ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§è‡§ï ‡§Ü‡§§‡•ç‡§Æ‡§æ\"']\n",
      "['\"Jaso engine ‡§µ‡§ø‡§∞‡§æ‡§∏‡§§ ‡§Æ‡•á‡§Ç ‡§π‡•à ‡§Ö‡§ó‡§≤‡•á ‡§ú‡§®‡§Æ‡•ã‡§Ç ‡§§‡§ï, ‡§á‡§∏‡§ï‡§æ ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§™ata ‡§π‡•à\"']\n",
      "['\"Abb bhai purani sacchi nahi ho sakte jinhe abb yahi hai, unhein naye owners ke dimag ka dahi kyun kar rahe hai\"']\n",
      "\"‡§Æ‡•Ä‡§≤‡•á‡§ú ‡§¨‡§§‡§æ‡§®‡•Ä ‡§•‡•Ä\"\n",
      "[\"‡§™‡•Å‡§∞‡§æ‡§®‡§æ ‡§π‡•Ä ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§®‡§ø‡§ó‡•ç‡§ó‡§æ\" (Hindi and English), \"Note: I've included the profanity/disrespectful word in the translation, as per your instructions. However, I strongly advise against using such language, as it can be offensive and disrespectful to others.\"]\n",
      "[\"\", \"Pulsar 220\" ‡§ï‡§æ ‡§®‡§æ‡§Æ \"Daddy\" ‡§π‡•à üòÇüòÇüòÇ]\n",
      "[\"New one looks like\", \"ninja 400\"]\n",
      "Comment: \"mere naam hai Roshan\"\n",
      "[\"‡§™‡•Å‡§∞‡§æ‡§®‡§æ ‡§ú‡•à‡§∏‡§æ ‡§∏‡§´‡•á‡§¶ ‡§π‡•à ‡§≤‡•á‡§ï‡§ø‡§® ‡§µ‡•ã ‡§ó‡•ã‡§≤‡•ç‡§¶ ‡§ú‡•à‡§∏‡§æ ‡§Æ‡§æ‡§®‡•á‡§ó‡§æ\"]\n",
      "[\"Old ‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä ‡§®‡§Ø‡§æ ‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä\" (Hindi script), \"\", \"Note: The phrase 'I dope' was translated to '‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä' which means 'I am addicted' or 'I am fond of' in Hindi. The rest of the phrase was translated as is, since it is in English.\"]\n",
      "[\"The old Karizma ‡§π‡•à ‡§ï‡•Å‡§õ ‡§ú‡•ã ‡§®‡§è ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç, ‡§è‡§ï ‡§Ü‡§§‡•ç‡§Æ‡§æ\"]\n",
      "[\"\\\"Jaso engine ‡§µ‡§ø‡§∞‡§æ‡§∏‡§§ ‡§Æ‡•á‡§Ç ‡§π‡•à ‡§Ö‡§ó‡§≤‡•á ‡§ú‡§®‡§Æ‡•ã‡§Ç ‡§§‡§ï, ‡§á‡§∏‡§ï‡§æ ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§§‡§æ ‡§π‡•à\\\"\"] \n",
      "[\" \"Abb bhai purani sacchi nahi ho sakte jinhe abb yahi hai, unhein naye owners ke dimag ka dahi kyun kar rahe hai\"\"]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Compile the template and set up the pipeline once\n",
    "template = \"\"\"You are a helpful assistant that translates the comment passed in mix of english + Indian languages to their original language phrase. \n",
    "Following are the conditions:\n",
    "If you find any English word just leave it as it is in English.\n",
    "I don't need  any word by word explanation, just need output in the translated language script.\n",
    "If you find any inappropriate or disrespectful word just output as \"Profanity/Disrespectful word/words included\"\n",
    "Please give only the original language phrase as shown in the example. \n",
    "Eg: comment: \"mere naam hai Roshan\",  \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à ‡§∞‡•ã‡§∂‡§®\"\n",
    "comment: \"nenu UST lo Panichestunna\", \"nenu UST ‡∞≤‡±ã ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ\"\n",
    "{comment} \n",
    "Output\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "pipeline = (\n",
    "    prompt\n",
    "    | ChatGroq(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))  \n",
    ")\n",
    "\n",
    "# Define the translation function\n",
    "def translation_to_source_script(comment):\n",
    "    print(pipeline.invoke({\"comment\": comment}))\n",
    "    return pipeline.invoke({\"comment\": comment})\n",
    "\n",
    "# Load the comments and apply the translation function\n",
    "comments = pd.read_excel(\"updated_comments.xlsx\", sheet_name='Comments')\n",
    "comments=comments.iloc[:10,:]\n",
    "tqdm.pandas()\n",
    "comments['translated_text'] = comments['Original_comment - \"Text\"'].apply(translation_to_source_script)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "pipeline = (\n",
    "    prompt\n",
    "    | ChatGroq(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\")[0])  # Directly access the first line of the output\n",
    ")\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that extracts the comment passed which has mix of language scripts, You have to just identify and get the Indian script part from it.\n",
    "Please give only the Indian language phrase as shown in the example. \n",
    "Eg: comment: \"mere naam hai Roshan ‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à ‡§∞‡•ã‡§∂‡§® \", original language phrase: \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à ‡§∞‡•ã‡§∂‡§®\"\n",
    "comment: \"nenu hero ga Panichestunna ‡∞®‡±á‡∞®‡±Å ‡∞π‡±Ä‡∞∞‡±ã‡∞ó‡∞æ ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å\", original language phrase: \"‡∞®‡±á‡∞®‡±Å ‡∞π‡±Ä‡∞∞‡±ã‡∞ó‡∞æ ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å\"\n",
    "{comment} \n",
    "Output\"\"\"\n",
    "\n",
    "comments['re-translated_text'] = comments['translated_text'].apply(translation_to_source_script)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CID</th>\n",
       "      <th>Original_comment - \"Text\"</th>\n",
       "      <th>comment</th>\n",
       "      <th>models</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>aspects</th>\n",
       "      <th>translated_text</th>\n",
       "      <th>re-translated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugxw3xwVOCGgtX_WCqN4AaABAg</td>\n",
       "      <td>Mileage bi btani thi</td>\n",
       "      <td>mileage bi tell to be</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'Mileage'}</td>\n",
       "      <td>\"‡§Æ‡•Ä‡§≤‡•á‡§ú ‡§¨‡§§‡§æ‡§®‡•Ä ‡§•‡•Ä\"</td>\n",
       "      <td>\"‡§Æ‡•Ä‡§≤‡•á‡§ú ‡§¨‡§§‡§æ‡§®‡•Ä ‡§•‡•Ä\" should be translated to \"‡§Æ‡•á‡§∞‡§æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ugy5CyM64xUhGNGZBNN4AaABAg</td>\n",
       "      <td>Old is gold nigga</td>\n",
       "      <td>old is gold nigga</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>positive</td>\n",
       "      <td>()</td>\n",
       "      <td>I cannot provide a translation for the term \"n...</td>\n",
       "      <td>Sure, I can help with that. Here's the transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugxh01ZYZ75etrRPXix4AaABAg</td>\n",
       "      <td>Daddy name Pulsar 220üòÇüòÇüòÇ</td>\n",
       "      <td>daddy name bajaj-pulsar</td>\n",
       "      <td>['bajaj-pulsar']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>()</td>\n",
       "      <td>Sure, I can help with that. The original langu...</td>\n",
       "      <td>\"Daddy ‡§ï‡§æ ‡§®‡§æ‡§Æ Pulsar 220üòÇüòÇüòÇ\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UgxOAJMxPG2s_u2p4FB4AaABAg</td>\n",
       "      <td>New one looks like ninja 400</td>\n",
       "      <td>new one looks like kawasaki-ninja</td>\n",
       "      <td>['kawasaki-ninja']</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'Design'}</td>\n",
       "      <td>original language phrase: \"‡§®‡§ø‡§®‡•ç‡§ú‡§æ 400 ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à\"</td>\n",
       "      <td>\"‡§®‡§ø‡§®‡•ç‡§ú‡§æ 400 ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UgwXN3os9eUvfpS04Vt4AaABAg</td>\n",
       "      <td>Old</td>\n",
       "      <td>old</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>()</td>\n",
       "      <td>Sure, I'd be happy to help you translate comme...</td>\n",
       "      <td>Example 1:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ugz0A-HC2Z15ktYp5-B4AaABAg</td>\n",
       "      <td>These \"old is gold\" NPCs will always choose th...</td>\n",
       "      <td>these old is gold npcs will always choose the ...</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>positive</td>\n",
       "      <td>()</td>\n",
       "      <td>\"‡§Ø‡•á ‡§¨‡§π‡•Å‡§§ ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§π‡•à‡§Ç ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ø‡•á ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Ü‡§™‡§ï‡•ã ‡§¨‡•á‡§ö‡§®‡•á ...</td>\n",
       "      <td>Original language phrase: \"‡§Ø‡•á ‡§¨‡§π‡•Å‡§§ ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§π‡•à‡§Ç ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UgzRgB1WHok3gW68IKF4AaABAg</td>\n",
       "      <td>Old i dope new is doopüòÖ</td>\n",
       "      <td>old i dope new is doop</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>()</td>\n",
       "      <td>\"Old ‡§Ö‡§¨ ‡§§‡§ï ‡§Æujhe doped ‡§•‡§æ ‡§®ew ‡§Ö‡§¨ doop ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§π‡•à\"</td>\n",
       "      <td>\"‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§Ö‡§¨ ‡§§‡§ï ‡§Æujhe doped ‡§•‡§æ ‡§®ew ‡§Ö‡§¨ doop ‡§π‡•ã ‡§ó‡§Ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UgxKn5fM5tdc8nV9Bs54AaABAg</td>\n",
       "      <td>The old Karizma has something the new one does...</td>\n",
       "      <td>the old hero-karizma has something the new one...</td>\n",
       "      <td>['hero-karizma']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>()</td>\n",
       "      <td>The old Karizma has something the new one does...</td>\n",
       "      <td>‡§ï‡§∞‡§ø‡§ú‡§Æ‡§æ ‡§ï‡•Ä ‡§¨‡•Å‡§ú‡•Å‡§∞‡•ç‡§ó‡•Ä ‡§®‡§è ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§è‡§ï ‡§Ü‡§§‡•ç‡§Æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UgxRKzLzhj750uNpqsJ4AaABAg</td>\n",
       "      <td>Jaso engine vs Indo-Euro engine, everyone know...</td>\n",
       "      <td>like engine vs indoeuro engine everyone knows ...</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'Engine'}</td>\n",
       "      <td>Original language phrase: \"‡§ú‡§æ‡§®‡•á ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§π‡•à ‡§ï‡§ø...</td>\n",
       "      <td>Original language phrase: \"‡§ú‡§æ‡§®‡•á ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§π‡•à ‡§ï‡§ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ugy5YOjzmXvg6PBrrYt4AaABAg</td>\n",
       "      <td>Abb bhai purani to le nhi skte jo ha abb yahi ...</td>\n",
       "      <td>abb bhai purani to le nhi skte jo ha abb yahi ...</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>()</td>\n",
       "      <td>Abb bhai purani \"old\" toys \"toys\" le \"take\" nh...</td>\n",
       "      <td>‡§Ö‡§¨ ‡§≠‡§æ‡§à ‡§™‡•Å‡§∞‡§æ‡§®‡•Ä \"old\" ‡§§‡•ã‡§Øz \"toys\" ‡§≤‡•á \"take\" ‡§®‡§π‡•Ä‡§Ç...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          CID  \\\n",
       "0  Ugxw3xwVOCGgtX_WCqN4AaABAg   \n",
       "1  Ugy5CyM64xUhGNGZBNN4AaABAg   \n",
       "2  Ugxh01ZYZ75etrRPXix4AaABAg   \n",
       "3  UgxOAJMxPG2s_u2p4FB4AaABAg   \n",
       "4  UgwXN3os9eUvfpS04Vt4AaABAg   \n",
       "5  Ugz0A-HC2Z15ktYp5-B4AaABAg   \n",
       "6  UgzRgB1WHok3gW68IKF4AaABAg   \n",
       "7  UgxKn5fM5tdc8nV9Bs54AaABAg   \n",
       "8  UgxRKzLzhj750uNpqsJ4AaABAg   \n",
       "9  Ugy5YOjzmXvg6PBrrYt4AaABAg   \n",
       "\n",
       "                           Original_comment - \"Text\"  \\\n",
       "0                               Mileage bi btani thi   \n",
       "1                                  Old is gold nigga   \n",
       "2                           Daddy name Pulsar 220üòÇüòÇüòÇ   \n",
       "3                       New one looks like ninja 400   \n",
       "4                                                Old   \n",
       "5  These \"old is gold\" NPCs will always choose th...   \n",
       "6                            Old i dope new is doopüòÖ   \n",
       "7  The old Karizma has something the new one does...   \n",
       "8  Jaso engine vs Indo-Euro engine, everyone know...   \n",
       "9  Abb bhai purani to le nhi skte jo ha abb yahi ...   \n",
       "\n",
       "                                             comment                  models  \\\n",
       "0                              mileage bi tell to be  ['No model mentioned']   \n",
       "1                                  old is gold nigga  ['No model mentioned']   \n",
       "2                            daddy name bajaj-pulsar        ['bajaj-pulsar']   \n",
       "3                  new one looks like kawasaki-ninja      ['kawasaki-ninja']   \n",
       "4                                                old  ['No model mentioned']   \n",
       "5  these old is gold npcs will always choose the ...  ['No model mentioned']   \n",
       "6                             old i dope new is doop  ['No model mentioned']   \n",
       "7  the old hero-karizma has something the new one...        ['hero-karizma']   \n",
       "8  like engine vs indoeuro engine everyone knows ...  ['No model mentioned']   \n",
       "9  abb bhai purani to le nhi skte jo ha abb yahi ...  ['No model mentioned']   \n",
       "\n",
       "  sentiment      aspects                                    translated_text  \\\n",
       "0   neutral  {'Mileage'}                                   \"‡§Æ‡•Ä‡§≤‡•á‡§ú ‡§¨‡§§‡§æ‡§®‡•Ä ‡§•‡•Ä\"   \n",
       "1  positive           ()  I cannot provide a translation for the term \"n...   \n",
       "2   neutral           ()  Sure, I can help with that. The original langu...   \n",
       "3  positive   {'Design'}   original language phrase: \"‡§®‡§ø‡§®‡•ç‡§ú‡§æ 400 ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à\"   \n",
       "4   neutral           ()  Sure, I'd be happy to help you translate comme...   \n",
       "5  positive           ()  \"‡§Ø‡•á ‡§¨‡§π‡•Å‡§§ ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§π‡•à‡§Ç ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ø‡•á ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Ü‡§™‡§ï‡•ã ‡§¨‡•á‡§ö‡§®‡•á ...   \n",
       "6   neutral           ()   \"Old ‡§Ö‡§¨ ‡§§‡§ï ‡§Æujhe doped ‡§•‡§æ ‡§®ew ‡§Ö‡§¨ doop ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§π‡•à\"   \n",
       "7   neutral           ()  The old Karizma has something the new one does...   \n",
       "8  positive   {'Engine'}  Original language phrase: \"‡§ú‡§æ‡§®‡•á ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§π‡•à ‡§ï‡§ø...   \n",
       "9   neutral           ()  Abb bhai purani \"old\" toys \"toys\" le \"take\" nh...   \n",
       "\n",
       "                                  re-translated_text  \n",
       "0  \"‡§Æ‡•Ä‡§≤‡•á‡§ú ‡§¨‡§§‡§æ‡§®‡•Ä ‡§•‡•Ä\" should be translated to \"‡§Æ‡•á‡§∞‡§æ...  \n",
       "1  Sure, I can help with that. Here's the transla...  \n",
       "2                       \"Daddy ‡§ï‡§æ ‡§®‡§æ‡§Æ Pulsar 220üòÇüòÇüòÇ\"  \n",
       "3                             \"‡§®‡§ø‡§®‡•ç‡§ú‡§æ 400 ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à\"  \n",
       "4                                         Example 1:  \n",
       "5  Original language phrase: \"‡§Ø‡•á ‡§¨‡§π‡•Å‡§§ ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§π‡•à‡§Ç ...  \n",
       "6  \"‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§Ö‡§¨ ‡§§‡§ï ‡§Æujhe doped ‡§•‡§æ ‡§®ew ‡§Ö‡§¨ doop ‡§π‡•ã ‡§ó‡§Ø...  \n",
       "7  ‡§ï‡§∞‡§ø‡§ú‡§Æ‡§æ ‡§ï‡•Ä ‡§¨‡•Å‡§ú‡•Å‡§∞‡•ç‡§ó‡•Ä ‡§®‡§è ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§è‡§ï ‡§Ü‡§§‡•ç‡§Æ...  \n",
       "8  Original language phrase: \"‡§ú‡§æ‡§®‡•á ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§π‡•à ‡§ï‡§ø...  \n",
       "9  ‡§Ö‡§¨ ‡§≠‡§æ‡§à ‡§™‡•Å‡§∞‡§æ‡§®‡•Ä \"old\" ‡§§‡•ã‡§Øz \"toys\" ‡§≤‡•á \"take\" ‡§®‡§π‡•Ä‡§Ç...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]/Users/dhanunjaikumar/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:991: UserWarning: Not enough free disk space to download the file. The expected file size is: 3812.26 MB. The target location /Users/dhanunjaikumar/.cache/huggingface/hub/models--sarvamai--OpenHathi-7B-Hi-v0.1-Base/blobs only has 3696.60 MB free disk space.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:42<00:00, 34.06s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡§æ‡§•‡•Ä ‡§π‡•Ç‡§Å‡•§\\n\\nI'm a good elephant, I'm a good elephant,\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base')\n",
    "model = LlamaForCausalLM.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base', torch_dtype=torch.bfloat16)\n",
    "\n",
    "prompt = \"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡§æ‡§•‡•Ä ‡§π‡•Ç‡§Å\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "('Could not convert tensor([    1,   835, 10567, 29901,  7338,  1461,  9565,   322, 19688,   515,\\n          278,  3440, 29889,  5648, 29903, 13380,   390,  5659,  8506,   620,\\n          744,   995, 13560,  2142, 29875,  1274,  4161,   447, 29875, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000]) with type Tensor: did not recognize Python value type when inferring an Arrow data type', 'Conversion failed for column input_ids with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 51\u001b[0m\n\u001b[1;32m     47\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m], df[\u001b[39m'\u001b[39m\u001b[39mresponse_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mdf\u001b[39m.\u001b[39mapply(extract_and_tokenize, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     49\u001b[0m \u001b[39m# Load dataset\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_pandas(df)  \u001b[39m# assuming your dataset is stored in a Pandas dataframe df\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m# # Apply the function to each row in the dataset\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m# dataset = dataset.map(extract_and_tokenize)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[39m# Set LoRA and BitsAndBytes configurations\u001b[39;00m\n\u001b[1;32m     62\u001b[0m peft_config \u001b[39m=\u001b[39m LoraConfig(\n\u001b[1;32m     63\u001b[0m     lora_alpha\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[1;32m     64\u001b[0m     lora_dropout\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     task_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCAUSAL_LM\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:875\u001b[0m, in \u001b[0;36mDataset.from_pandas\u001b[0;34m(cls, df, features, info, split, preserve_index)\u001b[0m\n\u001b[1;32m    873\u001b[0m     info \u001b[39m=\u001b[39m DatasetInfo()\n\u001b[1;32m    874\u001b[0m info\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m features\n\u001b[0;32m--> 875\u001b[0m table \u001b[39m=\u001b[39m InMemoryTable\u001b[39m.\u001b[39;49mfrom_pandas(\n\u001b[1;32m    876\u001b[0m     df\u001b[39m=\u001b[39;49mdf,\n\u001b[1;32m    877\u001b[0m     preserve_index\u001b[39m=\u001b[39;49mpreserve_index,\n\u001b[1;32m    878\u001b[0m )\n\u001b[1;32m    879\u001b[0m \u001b[39mif\u001b[39;00m features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[39m# more expensive cast than InMemoryTable.from_pandas(..., schema=features.arrow_schema)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[39m# needed to support the str to Audio conversion for instance\u001b[39;00m\n\u001b[1;32m    882\u001b[0m     table \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39mcast(features\u001b[39m.\u001b[39marrow_schema)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/datasets/table.py:720\u001b[0m, in \u001b[0;36mInMemoryTable.from_pandas\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    665\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pandas\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    666\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39m    Convert pandas.DataFrame to an Arrow Table.\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_pandas(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/pyarrow/table.pxi:4623\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/pyarrow/pandas_compat.py:629\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[39mfor\u001b[39;00m i, maybe_fut \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(arrays):\n\u001b[1;32m    628\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_fut, futures\u001b[39m.\u001b[39mFuture):\n\u001b[0;32m--> 629\u001b[0m             arrays[i] \u001b[39m=\u001b[39m maybe_fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    631\u001b[0m types \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mtype \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/pyarrow/pandas_compat.py:603\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid,\n\u001b[1;32m    599\u001b[0m         pa\u001b[39m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    600\u001b[0m         pa\u001b[39m.\u001b[39mArrowTypeError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    601\u001b[0m     e\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mConversion failed for column \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    602\u001b[0m                \u001b[39m.\u001b[39mformat(col\u001b[39m.\u001b[39mname, col\u001b[39m.\u001b[39mdtype),)\n\u001b[0;32m--> 603\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m field_nullable \u001b[39mand\u001b[39;00m result\u001b[39m.\u001b[39mnull_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    605\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mField \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m was non-nullable but pandas column \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mhad \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m null values\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mstr\u001b[39m(field),\n\u001b[1;32m    607\u001b[0m                                                  result\u001b[39m.\u001b[39mnull_count))\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/pyarrow/pandas_compat.py:597\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    594\u001b[0m     type_ \u001b[39m=\u001b[39m field\u001b[39m.\u001b[39mtype\n\u001b[1;32m    596\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 597\u001b[0m     result \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49marray(col, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49mtype_, from_pandas\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, safe\u001b[39m=\u001b[39;49msafe)\n\u001b[1;32m    598\u001b[0m \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid,\n\u001b[1;32m    599\u001b[0m         pa\u001b[39m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    600\u001b[0m         pa\u001b[39m.\u001b[39mArrowTypeError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    601\u001b[0m     e\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mConversion failed for column \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    602\u001b[0m                \u001b[39m.\u001b[39mformat(col\u001b[39m.\u001b[39mname, col\u001b[39m.\u001b[39mdtype),)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/pyarrow/array.pxi:358\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/pyarrow/array.pxi:85\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: ('Could not convert tensor([    1,   835, 10567, 29901,  7338,  1461,  9565,   322, 19688,   515,\\n          278,  3440, 29889,  5648, 29903, 13380,   390,  5659,  8506,   620,\\n          744,   995, 13560,  2142, 29875,  1274,  4161,   447, 29875, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\\n        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000]) with type Tensor: did not recognize Python value type when inferring an Arrow data type', 'Conversion failed for column input_ids with type object')"
     ]
    }
   ],
   "source": [
    "# # Install required libraries\n",
    "# !pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q datasets bitsandbytes einops\n",
    "# !pip install -q wandb\n",
    "\n",
    "# Import required libraries\n",
    "from datasets import load_dataset, Dataset\n",
    "from random import randrange\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to extract and tokenize the input text and response\n",
    "def extract_and_tokenize(row):\n",
    "    input_text = row['text'].split('\\\\n')[0]\n",
    "    response = row['text'].split('\\\\n')[1]\n",
    "    \n",
    "    input_ids = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=169,  # adjust the max length as needed\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids'].flatten()\n",
    "    \n",
    "    response_ids = tokenizer.encode_plus(\n",
    "        response,\n",
    "        max_length=158,  # adjust the max length as needed\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids'].flatten()\n",
    "    \n",
    "    return input_ids, response_ids\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"test_sft.csv\")\n",
    "# Apply the function to each row in the dataframe\n",
    "df['input_ids'], df['response_ids'] = zip(*df.apply(extract_and_tokenize, axis=1))\n",
    "\n",
    "df['input_ids'] = df['input_ids'].apply(lambda x: x.tolist())\n",
    "df['response_ids'] = df['response_ids'].apply(lambda x: x.tolist())\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df)  # assuming your dataset is stored in a Pandas dataframe df\n",
    "\n",
    "# # Apply the function to each row in the dataset\n",
    "# dataset = dataset.map(extract_and_tokenize)\n",
    "\n",
    "# Set model and tokenizer\n",
    "# model_name = \"sarvamai/OpenHathi-7B-Hi-v0.1-Base\"\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "# model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Set LoRA and BitsAndBytes configurations\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "# Log in to HF Hub\n",
    "login()\n",
    "\n",
    "# Log in to W&B\n",
    "wandb.login()\n",
    "\n",
    "# Set environment variable for W&B project\n",
    "%env WANDB_PROJECT=your_wandb_project_name  # replace with your W&B project name\n",
    "\n",
    "# Set training arguments\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=\"finetuned_model\",  # replace with your desired output dir\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=True,\n",
    "    report_to=\"wandb\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=lambda x: x,  # no formatting function needed in this case\n",
    "    args=trainingArgs,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pylora'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpylora\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllm_layers\u001b[39;00m \u001b[39mimport\u001b[39;00m LoRALayer, LoRAModule\n",
      "\u001b[1;32m      4\u001b[0m \u001b[39m# Create a custom dataset class to handle our tokenized data\u001b[39;00m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAspectSentimentDataset\u001b[39;00m(torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pylora'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Create a custom dataset class to handle our tokenized data\n",
    "class AspectSentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.df.iloc[idx, 1]\n",
    "        response_ids = self.df.iloc[idx, 2]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(response_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create a dataset instance and data loader\n",
    "dataset = AspectSentimentDataset(df, tokenizer, max_len=169)\n",
    "batch_size = 16\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load the pre-trained LlamaForCausalLM model\n",
    "\n",
    "# Add LoRA layers to the model\n",
    "lora_dim = 8\n",
    "lora_module = LoRAModule(r=lora_dim)\n",
    "model.transformer.output.LayerNorm = LoRALayer(lora_module)\n",
    "\n",
    "# Add adapters to the model\n",
    "adapter_dim = 64\n",
    "adapter_module = AdapterModule(adapter_dim)\n",
    "model.transformer.output.adapter = adapter_module\n",
    "\n",
    "# Set the device (CPU)\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):  # train for 5 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = loss_fn(outputs.logits.view(-1, 50264), labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
    "\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['text'].apply(lambda x: (x.split('\\\\n'))[0])).apply(len).max()\n",
    "#(df['text'].apply(lambda x: (x.split('\\\\n'))[1])).apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text'], dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"test_sft.csv\")\n",
    "df.text[0] \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define a function to extract and tokenize the input text and response\n",
    "def extract_and_tokenize(row):\n",
    "    input_text = row['text'].split('\\\\n')[0]\n",
    "    response = row['text'].split('\\\\n')[1]\n",
    "    \n",
    "    input_ids = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=169,  # adjust the max length as needed\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids'].flatten()\n",
    "    \n",
    "    response_ids = tokenizer.encode_plus(\n",
    "        response,\n",
    "        max_length=158,  # adjust the max length as needed\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids'].flatten()\n",
    "    \n",
    "    return input_ids, response_ids\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "df['input_ids'], df['response_ids'] = zip(*df.apply(extract_and_tokenize, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a custom dataset class to handle our tokenized data\n",
    "class AspectSentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.df.iloc[idx, 1]\n",
    "        response_ids = self.df.iloc[idx, 2]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(response_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create a dataset instance and data loader\n",
    "dataset = AspectSentimentDataset(df, tokenizer, max_len=169)\n",
    "batch_size = 16\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Set the device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):  # train for 5 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = loss_fn(outputs.logits.view(-1, 50264), labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
    "\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pylora'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpylora\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllm_layers\u001b[39;00m \u001b[39mimport\u001b[39;00m LoRALayer, LoRAModule\n\u001b[1;32m      4\u001b[0m \u001b[39m# Create a custom dataset class to handle our tokenized data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAspectSentimentDataset\u001b[39;00m(torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pylora'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Create a custom dataset class to handle our tokenized data\n",
    "class AspectSentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.df.iloc[idx, 1]\n",
    "        response_ids = self.df.iloc[idx, 2]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(response_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create a dataset instance and data loader\n",
    "dataset = AspectSentimentDataset(df, tokenizer, max_len=169)\n",
    "batch_size = 16\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load the pre-trained LlamaForCausalLM model\n",
    "\n",
    "# Add LoRA layers to the model\n",
    "lora_dim = 8\n",
    "lora_module = LoRAModule(r=lora_dim)\n",
    "model.transformer.output.LayerNorm = LoRALayer(lora_module)\n",
    "\n",
    "# Add adapters to the model\n",
    "adapter_dim = 64\n",
    "adapter_module = AdapterModule(adapter_dim)\n",
    "model.transformer.output.adapter = adapter_module\n",
    "\n",
    "# Set the device (CPU)\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):  # train for 5 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = loss_fn(outputs.logits.view(-1, 50264), labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
    "\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-2b.\n401 Client Error. (Request ID: Root=1-66bf2cda-0868f0da5f4849c2595999d0;e152cb05-32a2-486e-a83d-3f707674ab5a)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/unsloth/config.json.\nAccess to model google/gemma-2b is restricted. You must be authenticated to access it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/google/gemma-2b/resolve/unsloth/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    403\u001b[0m         path_or_repo_id,\n\u001b[1;32m    404\u001b[0m         filename,\n\u001b[1;32m    405\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    406\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    407\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    408\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    409\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    410\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    411\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    412\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    413\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    414\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    415\u001b[0m     )\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     \u001b[39mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1241\u001b[0m         \u001b[39m# Destination\u001b[39;49;00m\n\u001b[1;32m   1242\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1243\u001b[0m         \u001b[39m# File info\u001b[39;49;00m\n\u001b[1;32m   1244\u001b[0m         repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m   1245\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m   1246\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m   1247\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1248\u001b[0m         \u001b[39m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m         endpoint\u001b[39m=\u001b[39;49mendpoint,\n\u001b[1;32m   1250\u001b[0m         etag_timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1251\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1252\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1253\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1254\u001b[0m         \u001b[39m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1255\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1256\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1257\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1347\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[39m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[1;32m   1349\u001b[0m \u001b[39m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1853\u001b[0m     \u001b[39m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     \u001b[39mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1855\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1751\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1752\u001b[0m         url\u001b[39m=\u001b[39;49murl, proxies\u001b[39m=\u001b[39;49mproxies, timeout\u001b[39m=\u001b[39;49metag_timeout, headers\u001b[39m=\u001b[39;49mheaders, token\u001b[39m=\u001b[39;49mtoken\n\u001b[1;32m   1753\u001b[0m     )\n\u001b[1;32m   1754\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1673\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1674\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1675\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1676\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1677\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1678\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1679\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1680\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1681\u001b[0m )\n\u001b[1;32m   1682\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:376\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    377\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    378\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    379\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    380\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    381\u001b[0m     )\n\u001b[1;32m    383\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:400\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    399\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 400\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    401\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:321\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    318\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    319\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot access gated repo for url \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 321\u001b[0m     \u001b[39mraise\u001b[39;00m GatedRepoError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39melif\u001b[39;00m error_message \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAccess to this resource is disabled.\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-66bf2cda-0868f0da5f4849c2595999d0;e152cb05-32a2-486e-a83d-3f707674ab5a)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/unsloth/config.json.\nAccess to model google/gemma-2b is restricted. You must be authenticated to access it.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoPeftModelForCausalLM\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m AutoPeftModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mTelugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m hf_token\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhf_nDELizGbWCtaCiFwLNRfiWRNeoZyFdwaXs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mTelugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa-2.0\u001b[39m\u001b[39m\"\u001b[39m,load_in_4bit \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     token \u001b[39m=\u001b[39m hf_token)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/peft/auto.py:106\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, revision, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot infer the auto class from the config, please make sure that you are loading the correct model for your task type.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     )\n\u001b[0;32m--> 106\u001b[0m base_model \u001b[39m=\u001b[39m target_class\u001b[39m.\u001b[39;49mfrom_pretrained(base_model_path, revision\u001b[39m=\u001b[39;49mbase_model_revision, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    108\u001b[0m tokenizer_exists \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pretrained_model_name_or_path, TOKENIZER_CONFIG_NAME)):\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:524\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mquantization_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     _ \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mquantization_config\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 524\u001b[0m config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    525\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    526\u001b[0m     return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    527\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    528\u001b[0m     code_revision\u001b[39m=\u001b[39;49mcode_revision,\n\u001b[1;32m    529\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    530\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    531\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    532\u001b[0m )\n\u001b[1;32m    534\u001b[0m \u001b[39m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m kwargs_orig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:976\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    974\u001b[0m code_revision \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 976\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    977\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    978\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME) \u001b[39mif\u001b[39;00m gguf_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    690\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    691\u001b[0m         configuration_file,\n\u001b[1;32m    692\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    693\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    694\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    695\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    696\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    697\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    698\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    699\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    700\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    701\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:420\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m resolved_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    419\u001b[0m         \u001b[39mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    421\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to have access to it at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    425\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`token=<your_token>`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-2b.\n401 Client Error. (Request ID: Root=1-66bf2cda-0868f0da5f4849c2595999d0;e152cb05-32a2-486e-a83d-3f707674ab5a)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/unsloth/config.json.\nAccess to model google/gemma-2b is restricted. You must be authenticated to access it."
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa\"\n",
    ")\n",
    "hf_token= \"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\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa-2.0\",load_in_4bit = False,\n",
    "    token = hf_token)\n",
    "\n",
    "input_prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "input_text = input_prompt.format(\n",
    "        \"Tranlsate following sentence to Hindi.\", # instruction\n",
    "        \"This model is developed by Telugu LLM Labs\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "\n",
    "inputs = tokenizer([input_text], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 300, use_cache = True)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
