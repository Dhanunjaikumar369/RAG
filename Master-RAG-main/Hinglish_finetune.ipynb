{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'advanced-rag'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Mileage bataani thi\" (Hindi)']\n",
      "['\"Purana hi accha nigga\" (Hindi and English)', '', \"Note: I've included the profanity/disrespectful word in the translation, as per your instructions. However, I strongly advise against using such language, as it can be offensive and disrespectful to others.\"]\n",
      "['\"Daddy\" ‡§ï‡§æ ‡§®‡§æ‡§Æ \"Pulsar 220\" üòÇüòÇüòÇ', '', 'In this case, \"Daddy\" is being used as a term of endearment or admiration towards the Pulsar 220 motorcycle, so I have translated the rest of the sentence accordingly.']\n",
      "['For the given comment \"New one looks like ninja 400\", I will leave the English words as they are and provide the rest in Hindi:', '', '\"Naya ek jaisa hai ninja 400\"']\n",
      "[\"Sure, I'll do my best to translate the comments from a mix of English and Indian languages to their original language script. Here are some examples:\", '', 'Comment: \"mere naam hai Roshan\"', 'Output: \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à ‡§∞‡•ã‡§∂‡§®\"', '', 'Comment: \"nenu UST lo Panichestunna\"', 'Output: \"‡∞®‡±á‡∞®‡±Å UST ‡∞≤‡±ã ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ\"', '', 'Comment: \"my age is 25 years\"', 'Output: \"my age is 25 years\"', '', 'Comment: \"aapka naam kya hai?\"', 'Output: \"‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?\"', '', 'Comment: \"sundar kaandam\"', 'Output: \"‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§ï‡§æ‡§Ç‡§°‡§Æ\"', '', 'Comment: \"Your comment is very bad\"', 'Output: \"Your comment is very bad\"', '', 'Comment: \"tum log aise comments mat do\"', 'Output: \"‡§§‡•Å‡§Æ ‡§≤‡•ã‡§ó ‡§ê‡§∏‡•á comments ‡§Æ‡§§ ‡§¶‡•ã\"', '', 'Comment: \"sukaam hai aapki madam\"', 'Output: \"‡§∏‡•Å‡§ï‡§æ‡§Æ ‡§π‡•à ‡§Ü‡§™‡§ï‡•Ä madam\"', '', 'I hope this helps! Let me know if you have any other questions.']\n",
      "['\"purana jaisa safeda hai lekin woh gold jaisa maanegaa\"', '', '(Note: This is a translation of the English phrase \"old is gold\" in the comment. The rest of the sentence is already in English.)']\n",
      "['\"Old ‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä ‡§®‡§Ø‡§æ ‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä\" (Hindi script)', '', 'Note: The phrase \"I dope\" was translated to \"‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä\" which means \"I am addicted\" or \"I am fond of\" in Hindi. The rest of the phrase was translated as is, since it is in English.']\n",
      "['The old Karizma has something the new one doesn\\'t, a soul : \"‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§ï‡§æ‡§∞‡§ø‡§ú‡§Æ‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§π‡•à ‡§ú‡•ã ‡§®‡§è ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§è‡§ï ‡§Ü‡§§‡•ç‡§Æ‡§æ\"']\n",
      "['\"Jaso engine ‡§µ‡§ø‡§∞‡§æ‡§∏‡§§ ‡§Æ‡•á‡§Ç ‡§π‡•à ‡§Ö‡§ó‡§≤‡•á ‡§ú‡§®‡§Æ‡•ã‡§Ç ‡§§‡§ï, ‡§á‡§∏‡§ï‡§æ ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§™ata ‡§π‡•à\"']\n",
      "['\"Abb bhai purani sacchi nahi ho sakte jinhe abb yahi hai, unhein naye owners ke dimag ka dahi kyun kar rahe hai\"']\n",
      "\"‡§Æ‡•Ä‡§≤‡•á‡§ú ‡§¨‡§§‡§æ‡§®‡•Ä ‡§•‡•Ä\"\n",
      "[\"‡§™‡•Å‡§∞‡§æ‡§®‡§æ ‡§π‡•Ä ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§®‡§ø‡§ó‡•ç‡§ó‡§æ\" (Hindi and English), \"Note: I've included the profanity/disrespectful word in the translation, as per your instructions. However, I strongly advise against using such language, as it can be offensive and disrespectful to others.\"]\n",
      "[\"\", \"Pulsar 220\" ‡§ï‡§æ ‡§®‡§æ‡§Æ \"Daddy\" ‡§π‡•à üòÇüòÇüòÇ]\n",
      "[\"New one looks like\", \"ninja 400\"]\n",
      "Comment: \"mere naam hai Roshan\"\n",
      "[\"‡§™‡•Å‡§∞‡§æ‡§®‡§æ ‡§ú‡•à‡§∏‡§æ ‡§∏‡§´‡•á‡§¶ ‡§π‡•à ‡§≤‡•á‡§ï‡§ø‡§® ‡§µ‡•ã ‡§ó‡•ã‡§≤‡•ç‡§¶ ‡§ú‡•à‡§∏‡§æ ‡§Æ‡§æ‡§®‡•á‡§ó‡§æ\"]\n",
      "[\"Old ‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä ‡§®‡§Ø‡§æ ‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä\" (Hindi script), \"\", \"Note: The phrase 'I dope' was translated to '‡§Ö‡§π‡§Æ ‡§ö‡•Å‡§ü‡•ç‡§ü‡•Ä' which means 'I am addicted' or 'I am fond of' in Hindi. The rest of the phrase was translated as is, since it is in English.\"]\n",
      "[\"The old Karizma ‡§π‡•à ‡§ï‡•Å‡§õ ‡§ú‡•ã ‡§®‡§è ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç, ‡§è‡§ï ‡§Ü‡§§‡•ç‡§Æ‡§æ\"]\n",
      "[\"\\\"Jaso engine ‡§µ‡§ø‡§∞‡§æ‡§∏‡§§ ‡§Æ‡•á‡§Ç ‡§π‡•à ‡§Ö‡§ó‡§≤‡•á ‡§ú‡§®‡§Æ‡•ã‡§Ç ‡§§‡§ï, ‡§á‡§∏‡§ï‡§æ ‡§µ‡§ø‡§∞‡•ã‡§ß ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§§‡§æ ‡§π‡•à\\\"\"] \n",
      "[\" \"Abb bhai purani sacchi nahi ho sakte jinhe abb yahi hai, unhein naye owners ke dimag ka dahi kyun kar rahe hai\"\"]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Compile the template and set up the pipeline once\n",
    "template = \"\"\"You are a helpful assistant that translates the comment passed in mix of english + Indian languages to their original language phrase. \n",
    "Following are the conditions:\n",
    "If you find any English word just leave it as it is in English.\n",
    "I don't need  any word by word explanation, just need output in the translated language script.\n",
    "If you find any inappropriate or disrespectful word just output as \"Profanity/Disrespectful word/words included\"\n",
    "Please give only the original language phrase as shown in the example. \n",
    "Eg: comment: \"mere naam hai Roshan\",  \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à ‡§∞‡•ã‡§∂‡§®\"\n",
    "comment: \"nenu UST lo Panichestunna\", \"nenu UST ‡∞≤‡±ã ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ\"\n",
    "{comment} \n",
    "Output\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "pipeline = (\n",
    "    prompt\n",
    "    | ChatGroq(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))  \n",
    ")\n",
    "\n",
    "# Define the translation function\n",
    "def translation_to_source_script(comment):\n",
    "    print(pipeline.invoke({\"comment\": comment}))\n",
    "    return pipeline.invoke({\"comment\": comment})\n",
    "\n",
    "# Load the comments and apply the translation function\n",
    "comments = pd.read_excel(\"updated_comments.xlsx\", sheet_name='Comments')\n",
    "comments=comments.iloc[:10,:]\n",
    "tqdm.pandas()\n",
    "comments['translated_text'] = comments['Original_comment - \"Text\"'].apply(translation_to_source_script)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "pipeline = (\n",
    "    prompt\n",
    "    | ChatGroq(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\")[0])  # Directly access the first line of the output\n",
    ")\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that extracts the comment passed which has mix of language scripts, You have to just identify and get the Indian script part from it.\n",
    "Please give only the Indian language phrase as shown in the example. \n",
    "Eg: comment: \"mere naam hai Roshan ‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à ‡§∞‡•ã‡§∂‡§® \", original language phrase: \"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à ‡§∞‡•ã‡§∂‡§®\"\n",
    "comment: \"nenu hero ga Panichestunna ‡∞®‡±á‡∞®‡±Å ‡∞π‡±Ä‡∞∞‡±ã‡∞ó‡∞æ ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å\", original language phrase: \"‡∞®‡±á‡∞®‡±Å ‡∞π‡±Ä‡∞∞‡±ã‡∞ó‡∞æ ‡∞™‡∞®‡∞ø‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å\"\n",
    "{comment} \n",
    "Output\"\"\"\n",
    "\n",
    "comments['re-translated_text'] = comments['translated_text'].apply(translation_to_source_script)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CID</th>\n",
       "      <th>Original_comment - \"Text\"</th>\n",
       "      <th>comment</th>\n",
       "      <th>models</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>aspects</th>\n",
       "      <th>translated_text</th>\n",
       "      <th>re-translated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugxw3xwVOCGgtX_WCqN4AaABAg</td>\n",
       "      <td>Mileage bi btani thi</td>\n",
       "      <td>mileage bi tell to be</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'Mileage'}</td>\n",
       "      <td>\"‡§Æ‡•Ä‡§≤‡•á‡§ú ‡§¨‡§§‡§æ‡§®‡•Ä ‡§•‡•Ä\"</td>\n",
       "      <td>\"‡§Æ‡•Ä‡§≤‡•á‡§ú ‡§¨‡§§‡§æ‡§®‡•Ä ‡§•‡•Ä\" should be translated to \"‡§Æ‡•á‡§∞‡§æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ugy5CyM64xUhGNGZBNN4AaABAg</td>\n",
       "      <td>Old is gold nigga</td>\n",
       "      <td>old is gold nigga</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>positive</td>\n",
       "      <td>()</td>\n",
       "      <td>I cannot provide a translation for the term \"n...</td>\n",
       "      <td>Sure, I can help with that. Here's the transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugxh01ZYZ75etrRPXix4AaABAg</td>\n",
       "      <td>Daddy name Pulsar 220üòÇüòÇüòÇ</td>\n",
       "      <td>daddy name bajaj-pulsar</td>\n",
       "      <td>['bajaj-pulsar']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>()</td>\n",
       "      <td>Sure, I can help with that. The original langu...</td>\n",
       "      <td>\"Daddy ‡§ï‡§æ ‡§®‡§æ‡§Æ Pulsar 220üòÇüòÇüòÇ\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UgxOAJMxPG2s_u2p4FB4AaABAg</td>\n",
       "      <td>New one looks like ninja 400</td>\n",
       "      <td>new one looks like kawasaki-ninja</td>\n",
       "      <td>['kawasaki-ninja']</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'Design'}</td>\n",
       "      <td>original language phrase: \"‡§®‡§ø‡§®‡•ç‡§ú‡§æ 400 ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à\"</td>\n",
       "      <td>\"‡§®‡§ø‡§®‡•ç‡§ú‡§æ 400 ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UgwXN3os9eUvfpS04Vt4AaABAg</td>\n",
       "      <td>Old</td>\n",
       "      <td>old</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>()</td>\n",
       "      <td>Sure, I'd be happy to help you translate comme...</td>\n",
       "      <td>Example 1:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ugz0A-HC2Z15ktYp5-B4AaABAg</td>\n",
       "      <td>These \"old is gold\" NPCs will always choose th...</td>\n",
       "      <td>these old is gold npcs will always choose the ...</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>positive</td>\n",
       "      <td>()</td>\n",
       "      <td>\"‡§Ø‡•á ‡§¨‡§π‡•Å‡§§ ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§π‡•à‡§Ç ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ø‡•á ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Ü‡§™‡§ï‡•ã ‡§¨‡•á‡§ö‡§®‡•á ...</td>\n",
       "      <td>Original language phrase: \"‡§Ø‡•á ‡§¨‡§π‡•Å‡§§ ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§π‡•à‡§Ç ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UgzRgB1WHok3gW68IKF4AaABAg</td>\n",
       "      <td>Old i dope new is doopüòÖ</td>\n",
       "      <td>old i dope new is doop</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>()</td>\n",
       "      <td>\"Old ‡§Ö‡§¨ ‡§§‡§ï ‡§Æujhe doped ‡§•‡§æ ‡§®ew ‡§Ö‡§¨ doop ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§π‡•à\"</td>\n",
       "      <td>\"‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§Ö‡§¨ ‡§§‡§ï ‡§Æujhe doped ‡§•‡§æ ‡§®ew ‡§Ö‡§¨ doop ‡§π‡•ã ‡§ó‡§Ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UgxKn5fM5tdc8nV9Bs54AaABAg</td>\n",
       "      <td>The old Karizma has something the new one does...</td>\n",
       "      <td>the old hero-karizma has something the new one...</td>\n",
       "      <td>['hero-karizma']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>()</td>\n",
       "      <td>The old Karizma has something the new one does...</td>\n",
       "      <td>‡§ï‡§∞‡§ø‡§ú‡§Æ‡§æ ‡§ï‡•Ä ‡§¨‡•Å‡§ú‡•Å‡§∞‡•ç‡§ó‡•Ä ‡§®‡§è ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§è‡§ï ‡§Ü‡§§‡•ç‡§Æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UgxRKzLzhj750uNpqsJ4AaABAg</td>\n",
       "      <td>Jaso engine vs Indo-Euro engine, everyone know...</td>\n",
       "      <td>like engine vs indoeuro engine everyone knows ...</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'Engine'}</td>\n",
       "      <td>Original language phrase: \"‡§ú‡§æ‡§®‡•á ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§π‡•à ‡§ï‡§ø...</td>\n",
       "      <td>Original language phrase: \"‡§ú‡§æ‡§®‡•á ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§π‡•à ‡§ï‡§ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ugy5YOjzmXvg6PBrrYt4AaABAg</td>\n",
       "      <td>Abb bhai purani to le nhi skte jo ha abb yahi ...</td>\n",
       "      <td>abb bhai purani to le nhi skte jo ha abb yahi ...</td>\n",
       "      <td>['No model mentioned']</td>\n",
       "      <td>neutral</td>\n",
       "      <td>()</td>\n",
       "      <td>Abb bhai purani \"old\" toys \"toys\" le \"take\" nh...</td>\n",
       "      <td>‡§Ö‡§¨ ‡§≠‡§æ‡§à ‡§™‡•Å‡§∞‡§æ‡§®‡•Ä \"old\" ‡§§‡•ã‡§Øz \"toys\" ‡§≤‡•á \"take\" ‡§®‡§π‡•Ä‡§Ç...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          CID  \\\n",
       "0  Ugxw3xwVOCGgtX_WCqN4AaABAg   \n",
       "1  Ugy5CyM64xUhGNGZBNN4AaABAg   \n",
       "2  Ugxh01ZYZ75etrRPXix4AaABAg   \n",
       "3  UgxOAJMxPG2s_u2p4FB4AaABAg   \n",
       "4  UgwXN3os9eUvfpS04Vt4AaABAg   \n",
       "5  Ugz0A-HC2Z15ktYp5-B4AaABAg   \n",
       "6  UgzRgB1WHok3gW68IKF4AaABAg   \n",
       "7  UgxKn5fM5tdc8nV9Bs54AaABAg   \n",
       "8  UgxRKzLzhj750uNpqsJ4AaABAg   \n",
       "9  Ugy5YOjzmXvg6PBrrYt4AaABAg   \n",
       "\n",
       "                           Original_comment - \"Text\"  \\\n",
       "0                               Mileage bi btani thi   \n",
       "1                                  Old is gold nigga   \n",
       "2                           Daddy name Pulsar 220üòÇüòÇüòÇ   \n",
       "3                       New one looks like ninja 400   \n",
       "4                                                Old   \n",
       "5  These \"old is gold\" NPCs will always choose th...   \n",
       "6                            Old i dope new is doopüòÖ   \n",
       "7  The old Karizma has something the new one does...   \n",
       "8  Jaso engine vs Indo-Euro engine, everyone know...   \n",
       "9  Abb bhai purani to le nhi skte jo ha abb yahi ...   \n",
       "\n",
       "                                             comment                  models  \\\n",
       "0                              mileage bi tell to be  ['No model mentioned']   \n",
       "1                                  old is gold nigga  ['No model mentioned']   \n",
       "2                            daddy name bajaj-pulsar        ['bajaj-pulsar']   \n",
       "3                  new one looks like kawasaki-ninja      ['kawasaki-ninja']   \n",
       "4                                                old  ['No model mentioned']   \n",
       "5  these old is gold npcs will always choose the ...  ['No model mentioned']   \n",
       "6                             old i dope new is doop  ['No model mentioned']   \n",
       "7  the old hero-karizma has something the new one...        ['hero-karizma']   \n",
       "8  like engine vs indoeuro engine everyone knows ...  ['No model mentioned']   \n",
       "9  abb bhai purani to le nhi skte jo ha abb yahi ...  ['No model mentioned']   \n",
       "\n",
       "  sentiment      aspects                                    translated_text  \\\n",
       "0   neutral  {'Mileage'}                                   \"‡§Æ‡•Ä‡§≤‡•á‡§ú ‡§¨‡§§‡§æ‡§®‡•Ä ‡§•‡•Ä\"   \n",
       "1  positive           ()  I cannot provide a translation for the term \"n...   \n",
       "2   neutral           ()  Sure, I can help with that. The original langu...   \n",
       "3  positive   {'Design'}   original language phrase: \"‡§®‡§ø‡§®‡•ç‡§ú‡§æ 400 ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à\"   \n",
       "4   neutral           ()  Sure, I'd be happy to help you translate comme...   \n",
       "5  positive           ()  \"‡§Ø‡•á ‡§¨‡§π‡•Å‡§§ ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§π‡•à‡§Ç ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ø‡•á ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Ü‡§™‡§ï‡•ã ‡§¨‡•á‡§ö‡§®‡•á ...   \n",
       "6   neutral           ()   \"Old ‡§Ö‡§¨ ‡§§‡§ï ‡§Æujhe doped ‡§•‡§æ ‡§®ew ‡§Ö‡§¨ doop ‡§π‡•ã ‡§ó‡§Ø‡§æ ‡§π‡•à\"   \n",
       "7   neutral           ()  The old Karizma has something the new one does...   \n",
       "8  positive   {'Engine'}  Original language phrase: \"‡§ú‡§æ‡§®‡•á ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§π‡•à ‡§ï‡§ø...   \n",
       "9   neutral           ()  Abb bhai purani \"old\" toys \"toys\" le \"take\" nh...   \n",
       "\n",
       "                                  re-translated_text  \n",
       "0  \"‡§Æ‡•Ä‡§≤‡•á‡§ú ‡§¨‡§§‡§æ‡§®‡•Ä ‡§•‡•Ä\" should be translated to \"‡§Æ‡•á‡§∞‡§æ...  \n",
       "1  Sure, I can help with that. Here's the transla...  \n",
       "2                       \"Daddy ‡§ï‡§æ ‡§®‡§æ‡§Æ Pulsar 220üòÇüòÇüòÇ\"  \n",
       "3                             \"‡§®‡§ø‡§®‡•ç‡§ú‡§æ 400 ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à\"  \n",
       "4                                         Example 1:  \n",
       "5  Original language phrase: \"‡§Ø‡•á ‡§¨‡§π‡•Å‡§§ ‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§π‡•à‡§Ç ...  \n",
       "6  \"‡§™‡•Å‡§∞‡§æ‡§®‡•á ‡§Ö‡§¨ ‡§§‡§ï ‡§Æujhe doped ‡§•‡§æ ‡§®ew ‡§Ö‡§¨ doop ‡§π‡•ã ‡§ó‡§Ø...  \n",
       "7  ‡§ï‡§∞‡§ø‡§ú‡§Æ‡§æ ‡§ï‡•Ä ‡§¨‡•Å‡§ú‡•Å‡§∞‡•ç‡§ó‡•Ä ‡§®‡§è ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§è‡§ï ‡§Ü‡§§‡•ç‡§Æ...  \n",
       "8  Original language phrase: \"‡§ú‡§æ‡§®‡•á ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§Æ‡§æ‡§® ‡§π‡•à ‡§ï‡§ø...  \n",
       "9  ‡§Ö‡§¨ ‡§≠‡§æ‡§à ‡§™‡•Å‡§∞‡§æ‡§®‡•Ä \"old\" ‡§§‡•ã‡§Øz \"toys\" ‡§≤‡•á \"take\" ‡§®‡§π‡•Ä‡§Ç...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]/Users/dhanunjaikumar/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:991: UserWarning: Not enough free disk space to download the file. The expected file size is: 3812.26 MB. The target location /Users/dhanunjaikumar/.cache/huggingface/hub/models--sarvamai--OpenHathi-7B-Hi-v0.1-Base/blobs only has 3696.60 MB free disk space.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:42<00:00, 34.06s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡§æ‡§•‡•Ä ‡§π‡•Ç‡§Å‡•§\\n\\nI'm a good elephant, I'm a good elephant,\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base')\n",
    "model = LlamaForCausalLM.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base', torch_dtype=torch.bfloat16)\n",
    "\n",
    "prompt = \"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡§æ‡§•‡•Ä ‡§π‡•Ç‡§Å\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required libraries\n",
    "# !pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q datasets bitsandbytes einops\n",
    "# !pip install -q wandb\n",
    "\n",
    "# Import required libraries\n",
    "from datasets import load_dataset, Dataset\n",
    "from random import randrange\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base')\n",
    "model = LlamaForCausalLM.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base', torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "# Define a function to extract and tokenize the input text and response\n",
    "def extract_and_tokenize(row):\n",
    "    input_text = row['text'].split('\\\\n')[0]\n",
    "    response = row['text'].split('\\\\n')[1]\n",
    "    \n",
    "    input_ids = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=169,  # adjust the max length as needed\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids'].flatten()\n",
    "    \n",
    "    response_ids = tokenizer.encode_plus(\n",
    "        response,\n",
    "        max_length=158,  # adjust the max length as needed\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids'].flatten()\n",
    "    \n",
    "    return input_ids, response_ids\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"test_sft.csv\")\n",
    "# Apply the function to each row in the dataframe\n",
    "df['input_ids'], df['response_ids'] = zip(*df.apply(extract_and_tokenize, axis=1))\n",
    "\n",
    "df['input_ids'] = df['input_ids'].apply(lambda x: x.tolist())\n",
    "df['response_ids'] = df['response_ids'].apply(lambda x: x.tolist())\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df)  # assuming your dataset is stored in a Pandas dataframe df\n",
    "\n",
    "# # Apply the function to each row in the dataset\n",
    "# dataset = dataset.map(extract_and_tokenize)\n",
    "\n",
    "# Set model and tokenizer\n",
    "# model_name = \"sarvamai/OpenHathi-7B-Hi-v0.1-Base\"\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "# model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Set LoRA and BitsAndBytes configurations\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "# Log in to HF Hub\n",
    "notebook_login()\n",
    "\n",
    "# Log in to W&B\n",
    "# wandb.login()\n",
    "\n",
    "# # Set environment variable for W&B project\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"my-awesome-project\",\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"CIFAR-100\",\n",
    "#     \"epochs\": 10,\n",
    "#     }\n",
    "# )\n",
    "# %env WANDB_PROJECT=your_wandb_project_name  # replace with your W&B project name\n",
    "\n",
    "# Set training arguments\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=\"finetuned_model\",  # replace with your desired output dir\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    disable_tqdm=True,\n",
    "    #report_to=\"wandb\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=lambda x: x,  # no formatting function needed in this case\n",
    "    args=trainingArgs,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pylora'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpylora\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllm_layers\u001b[39;00m \u001b[39mimport\u001b[39;00m LoRALayer, LoRAModule\n",
      "\u001b[1;32m      4\u001b[0m \u001b[39m# Create a custom dataset class to handle our tokenized data\u001b[39;00m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAspectSentimentDataset\u001b[39;00m(torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pylora'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Create a custom dataset class to handle our tokenized data\n",
    "class AspectSentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.df.iloc[idx, 1]\n",
    "        response_ids = self.df.iloc[idx, 2]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(response_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create a dataset instance and data loader\n",
    "dataset = AspectSentimentDataset(df, tokenizer, max_len=169)\n",
    "batch_size = 16\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load the pre-trained LlamaForCausalLM model\n",
    "\n",
    "# Add LoRA layers to the model\n",
    "lora_dim = 8\n",
    "lora_module = LoRAModule(r=lora_dim)\n",
    "model.transformer.output.LayerNorm = LoRALayer(lora_module)\n",
    "\n",
    "# Add adapters to the model\n",
    "adapter_dim = 64\n",
    "adapter_module = AdapterModule(adapter_dim)\n",
    "model.transformer.output.adapter = adapter_module\n",
    "\n",
    "# Set the device (CPU)\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):  # train for 5 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = loss_fn(outputs.logits.view(-1, 50264), labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
    "\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['text'].apply(lambda x: (x.split('\\\\n'))[0])).apply(len).max()\n",
    "#(df['text'].apply(lambda x: (x.split('\\\\n'))[1])).apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text'], dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"test_sft.csv\")\n",
    "df.text[0] \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define a function to extract and tokenize the input text and response\n",
    "def extract_and_tokenize(row):\n",
    "    input_text = row['text'].split('\\\\n')[0]\n",
    "    response = row['text'].split('\\\\n')[1]\n",
    "    \n",
    "    input_ids = tokenizer.encode_plus(\n",
    "        input_text,\n",
    "        max_length=169,  # adjust the max length as needed\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids'].flatten()\n",
    "    \n",
    "    response_ids = tokenizer.encode_plus(\n",
    "        response,\n",
    "        max_length=158,  # adjust the max length as needed\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids'].flatten()\n",
    "    \n",
    "    return input_ids, response_ids\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "df['input_ids'], df['response_ids'] = zip(*df.apply(extract_and_tokenize, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a custom dataset class to handle our tokenized data\n",
    "class AspectSentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.df.iloc[idx, 1]\n",
    "        response_ids = self.df.iloc[idx, 2]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(response_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create a dataset instance and data loader\n",
    "dataset = AspectSentimentDataset(df, tokenizer, max_len=169)\n",
    "batch_size = 16\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Set the device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):  # train for 5 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = loss_fn(outputs.logits.view(-1, 50264), labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
    "\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pylora'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpylora\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllm_layers\u001b[39;00m \u001b[39mimport\u001b[39;00m LoRALayer, LoRAModule\n\u001b[1;32m      4\u001b[0m \u001b[39m# Create a custom dataset class to handle our tokenized data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAspectSentimentDataset\u001b[39;00m(torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pylora'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Create a custom dataset class to handle our tokenized data\n",
    "class AspectSentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.df.iloc[idx, 1]\n",
    "        response_ids = self.df.iloc[idx, 2]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(response_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create a dataset instance and data loader\n",
    "dataset = AspectSentimentDataset(df, tokenizer, max_len=169)\n",
    "batch_size = 16\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load the pre-trained LlamaForCausalLM model\n",
    "\n",
    "# Add LoRA layers to the model\n",
    "lora_dim = 8\n",
    "lora_module = LoRAModule(r=lora_dim)\n",
    "model.transformer.output.LayerNorm = LoRALayer(lora_module)\n",
    "\n",
    "# Add adapters to the model\n",
    "adapter_dim = 64\n",
    "adapter_module = AdapterModule(adapter_dim)\n",
    "model.transformer.output.adapter = adapter_module\n",
    "\n",
    "# Set the device (CPU)\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):  # train for 5 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = loss_fn(outputs.logits.view(-1, 50264), labels.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
    "\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-2b.\n403 Client Error. (Request ID: Root=1-66c340be-5fa340a93bc174ce500717f6;2f7be0d1-8407-4f48-8565-5957bdf6d22b)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/unsloth/config.json.\nAccess to model google/gemma-2b is restricted and you are not in the authorized list. Visit https://huggingface.co/google/gemma-2b to ask for access.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/google/gemma-2b/resolve/unsloth/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    403\u001b[0m         path_or_repo_id,\n\u001b[1;32m    404\u001b[0m         filename,\n\u001b[1;32m    405\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    406\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    407\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    408\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    409\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    410\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    411\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    412\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    413\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    414\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    415\u001b[0m     )\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     \u001b[39mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1241\u001b[0m         \u001b[39m# Destination\u001b[39;49;00m\n\u001b[1;32m   1242\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1243\u001b[0m         \u001b[39m# File info\u001b[39;49;00m\n\u001b[1;32m   1244\u001b[0m         repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m   1245\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m   1246\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m   1247\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1248\u001b[0m         \u001b[39m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m         endpoint\u001b[39m=\u001b[39;49mendpoint,\n\u001b[1;32m   1250\u001b[0m         etag_timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1251\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1252\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1253\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1254\u001b[0m         \u001b[39m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1255\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1256\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1257\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1347\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[39m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[1;32m   1349\u001b[0m \u001b[39m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1853\u001b[0m     \u001b[39m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     \u001b[39mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1855\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1751\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1752\u001b[0m         url\u001b[39m=\u001b[39;49murl, proxies\u001b[39m=\u001b[39;49mproxies, timeout\u001b[39m=\u001b[39;49metag_timeout, headers\u001b[39m=\u001b[39;49mheaders, token\u001b[39m=\u001b[39;49mtoken\n\u001b[1;32m   1753\u001b[0m     )\n\u001b[1;32m   1754\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1673\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1674\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1675\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1676\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1677\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1678\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1679\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1680\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1681\u001b[0m )\n\u001b[1;32m   1682\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:376\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    377\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    378\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    379\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    380\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    381\u001b[0m     )\n\u001b[1;32m    383\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:400\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    399\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 400\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    401\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:321\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    318\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    319\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot access gated repo for url \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 321\u001b[0m     \u001b[39mraise\u001b[39;00m GatedRepoError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39melif\u001b[39;00m error_message \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAccess to this resource is disabled.\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-66c340be-5fa340a93bc174ce500717f6;2f7be0d1-8407-4f48-8565-5957bdf6d22b)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/unsloth/config.json.\nAccess to model google/gemma-2b is restricted and you are not in the authorized list. Visit https://huggingface.co/google/gemma-2b to ask for access.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoPeftModelForCausalLM\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m AutoPeftModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mTelugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m hf_token\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhf_nDELizGbWCtaCiFwLNRfiWRNeoZyFdwaXs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mTelugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa-2.0\u001b[39m\u001b[39m\"\u001b[39m,load_in_4bit \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     token \u001b[39m=\u001b[39m hf_token)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/peft/auto.py:106\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, revision, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot infer the auto class from the config, please make sure that you are loading the correct model for your task type.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     )\n\u001b[0;32m--> 106\u001b[0m base_model \u001b[39m=\u001b[39m target_class\u001b[39m.\u001b[39;49mfrom_pretrained(base_model_path, revision\u001b[39m=\u001b[39;49mbase_model_revision, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    108\u001b[0m tokenizer_exists \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pretrained_model_name_or_path, TOKENIZER_CONFIG_NAME)):\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:524\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mquantization_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     _ \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mquantization_config\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 524\u001b[0m config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    525\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    526\u001b[0m     return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    527\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    528\u001b[0m     code_revision\u001b[39m=\u001b[39;49mcode_revision,\n\u001b[1;32m    529\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    530\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    531\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    532\u001b[0m )\n\u001b[1;32m    534\u001b[0m \u001b[39m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m kwargs_orig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:976\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    974\u001b[0m code_revision \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 976\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    977\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    978\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME) \u001b[39mif\u001b[39;00m gguf_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    690\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    691\u001b[0m         configuration_file,\n\u001b[1;32m    692\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    693\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    694\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    695\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    696\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    697\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    698\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    699\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    700\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    701\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:420\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m resolved_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    419\u001b[0m         \u001b[39mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    421\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to have access to it at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    425\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`token=<your_token>`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-2b.\n403 Client Error. (Request ID: Root=1-66c340be-5fa340a93bc174ce500717f6;2f7be0d1-8407-4f48-8565-5957bdf6d22b)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/unsloth/config.json.\nAccess to model google/gemma-2b is restricted and you are not in the authorized list. Visit https://huggingface.co/google/gemma-2b to ask for access."
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa\"\n",
    ")\n",
    "hf_token= \"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\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa-2.0\",load_in_4bit = False,\n",
    "    token = hf_token)\n",
    "\n",
    "input_prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "input_text = input_prompt.format(\n",
    "        \"Tranlsate following sentence to Hindi.\", # instruction\n",
    "        \"This model is developed by Telugu LLM Labs\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "\n",
    "inputs = tokenizer([input_text], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 300, use_cache = True)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fba68e776b4aaaba3d9fa453acb231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 8.90 GB, other allocations: 704.00 KB, max allowed: 9.07 GB). Tried to allocate 501.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m PYTORCH_MPS_HIGH_WATERMARK_RATIO\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 3\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msarvamai/sarvam-2b-v0.5\u001b[39;49m\u001b[39m'\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m pipe(\u001b[39m'\u001b[39m\u001b[39m‡§≠‡§æ‡§∞‡§§ ‡§ï‡•á ‡§™‡•ç‡§∞‡§•‡§Æ ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä\u001b[39m\u001b[39m'\u001b[39m, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, repetition_penalty\u001b[39m=\u001b[39m\u001b[39m1.2\u001b[39m)[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[39m# '‡§≠‡§æ‡§∞‡§§ ‡§ï‡•á ‡§™‡•ç‡§∞‡§•‡§Æ ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§ú‡§µ‡§æ‡§π‡§∞‡§≤‡§æ‡§≤ ‡§®‡•á‡§π‡§∞‡•Ç ‡§•‡•á‡•§\\n\\n'\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1097\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1095\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m device\n\u001b[0;32m-> 1097\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline_class(model\u001b[39m=\u001b[39;49mmodel, framework\u001b[39m=\u001b[39;49mframework, task\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:96\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 96\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_model_type(\n\u001b[1;32m     98\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mprefix\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_params:\n\u001b[1;32m    101\u001b[0m         \u001b[39m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[39m# as a \"default\".\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         \u001b[39m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[39m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:897\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[39m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    892\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    894\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m    895\u001b[0m     \u001b[39mand\u001b[39;00m hf_device_map \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    896\u001b[0m ):\n\u001b[0;32m--> 897\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    899\u001b[0m \u001b[39m# Update config and generation_config with task specific parameters\u001b[39;00m\n\u001b[1;32m    900\u001b[0m task_specific_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtask_specific_params\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2883\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2878\u001b[0m     \u001b[39mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m         )\n\u001b[0;32m-> 2883\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    782\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[39m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/git_test/RAG/Master-RAG-main/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[39m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m   1161\u001b[0m         device,\n\u001b[1;32m   1162\u001b[0m         dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1163\u001b[0m         non_blocking,\n\u001b[1;32m   1164\u001b[0m     )\n\u001b[1;32m   1165\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(e) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot copy out of meta tensor; no data!\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 8.90 GB, other allocations: 704.00 KB, max allowed: 9.07 GB). Tried to allocate 501.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(model='sarvamai/sarvam-2b-v0.5', device=0)\n",
    "pipe('‡§≠‡§æ‡§∞‡§§ ‡§ï‡•á ‡§™‡•ç‡§∞‡§•‡§Æ ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä', max_new_tokens=15, temperature=0.1, repetition_penalty=1.2)[0]['generated_text']\n",
    "# '‡§≠‡§æ‡§∞‡§§ ‡§ï‡•á ‡§™‡•ç‡§∞‡§•‡§Æ ‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§ú‡§µ‡§æ‡§π‡§∞‡§≤‡§æ‡§≤ ‡§®‡•á‡§π‡§∞‡•Ç ‡§•‡•á‡•§\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
